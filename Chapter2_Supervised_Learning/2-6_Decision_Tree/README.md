## 2-6_Decision_Tree

### 概要
- <b>決定木はモデルの解釈性が高い手法であるため，モデル構成や中身，特徴を理解するための手段としてもよく用いられる．</b>
   - 樹木状のモデルを作成してクラス分類をするという決定木の性質は，モデルの出力結果に対する説明性を求められる場合に有効になる．
   - <b>特徴量の重要度はモデルの解釈性を高めるだけでなく，決定木以外の機械学習アルゴリズムを使用する際の特徴量設計にも有効．</b>
      - <b>手元にあるデータをやみくもに複雑な機械学習モデルの与えるのではなく，事前に決定木を使って重要度の高いまたは低い特徴量を把握して特徴量設計できる．</b>
      - この特徴量評価プロセスを踏むことにより，データに対する理解が深まり，データ分析方針の整理・再検討を早い段階で実施できる．
- 決定木は，情報利得（親ノードから子ノードへどれだけうまく分割できたかを示す指標）が最大になるように条件判定を繰り返す分類手法．
   - <b>情報利得を最大にするということは，言い換えると，分割後の子ノードに含まれるデータのばらつきを最小にすること．</b>
   - <b>データのばらつきは，異なるクラスのサンプルがノードにどれだけ混ざっているかを表す不純度として定量化できる．（サンプルが特定のクラスに集中するほど不純度の値は小さくなる）</b>
- 決定木では特徴量に関するある一連の質問（条件判定）をもとにした二者択一の分岐を繰り返して入力データを特定のクラスに分類する手法．
   - 分類に効果的な条件判定を繰り返して分類問題を解く．
- 決定木ではデータセットの準備段階で特徴量を正規化する必要がない．
   - なぜなら，入力データを分割する際に用いられる分割条件は特定の1つの特徴量に着目して作成され，特徴量間のスケールの違いによる影響を受けないため．