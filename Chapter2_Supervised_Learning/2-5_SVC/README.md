## 2-5_SVC

### 概要
- <b>機械学習で分類問題を解決したい場合にはじめに試すアルゴリズムは，ロジスティック回帰もしくはSVC(Support Vector Machine Classification)が一般的．</b>
- SVCには，LinearSVC と KernelSVC がある．
   - KernelSVC は，非線形な境界線（曲線や曲面）を生み出すSVC．
      - 最近は，非線形に分離したい複雑なケースでは Kernel SVC よりも，ランダムフォレストやその派生手法である勾配ブースティング決定木（GBDT: Gradient Boosting Decision Tree）のほうが頻繁に使用されている．
- SVCは，境界線の最近傍のデータに着目することで，２つのメリットが得られる：
   1. 訓練データをきれいに分離する境界線を学習できる点
      - マージン最大化では境界線からのマージンを考えるので，ロジスティック回帰のように訓練データで誤分類が発生しない．（ただし，誤分類なく分離できる境界線が存在する訓練データの場合に限る．）
   2. 汎化性能が高い点
      - SVCはマージン，すなわち境界線から各クラスの最近傍にあるデータ（サポートベクトル）までの距離が最大になる場所に線を引くので，テストデータがサポートベクトルよりも少しばかり境界線の近くにあったとしても，誤分類する可能性が低くなる．
         - <b>ロジスティック回帰：訓練データの全部の誤差の総和を利用して境界線を引く</b>
         - <b>SVC：境界線最近傍のデータのみに着目する（高い汎化性能を生み出す）</b>

### ハードマージンSVC
- 訓練データに対して線形な境界線で誤分類なくきれいに訓練データを分離できるケースをハードマージンSVCと呼ぶ．

### ソフトマージンSVC
- 線形分離できないデータに対して行うSVCをソフトマージンSVCと呼ぶ．
- ハードマージンSVCと同様にマージン最大化を行うが，新たにペナルティという概念を導入する．
   - ペナルティとは，「サポートベクトルよりも境界線の逆側にあるデータ」と「サポートベクトル」との距離のこと．
- ソフトマージンSVCはマージンを最大化しつつ，ペナルティの総和の最小化を行う．
- <b>訓練データが確実に線形分離できるケースは現実的にはほぼあり得ないため，SVCと言えば基本的に，誤分類データにペナルティを与えるソフトマージンSVCを意味する．</b>