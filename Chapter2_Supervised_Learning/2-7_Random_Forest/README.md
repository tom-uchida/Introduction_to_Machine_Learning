## 2-7_Random_Forest

### 概要
- ランダムフォレストは，決定木をベースにした応用手法．
   - 決定木が持つモデルの解釈性が高い構造を取り入れつつ，かつ，決定木に比べて高い性能が出やすいアルゴリズム．
- ランダムフォレストは<b>決定木を複数作成し，各決定木の出力の多数決で分類結果を決める<b>手法．
   - アンサンブル手法とは，複数のモデルを組み合わせてお互いの欠点を補い合う1つの強いモデルを作成する方法．
   - 複数の決定木を組み合わせたモデルであるため，決定木に比べてモデルの解釈性は低下する．
   - しかし，決定木と同様に特徴量の重要度を定量化できるため，モデルの予測結果に対してどの特徴量が効いているかについては求めることができる．
- ランダムフォレストは<b>2つのランダム性（データ抽出のランダム性と特徴量抽出のランダム性）</b>を取り入れている．

### アルゴリズム
- 通常の決定木は，訓練データに対して情報利得が最大となる分割条件を選択して木を成長させていくため，訓練データに過学習しやすい傾向にある．
   - この問題に対処するために，ランダムフォレストでは2つのランダム性を取り入れる：
      1. <b>データのランダム性</b>
         - 複数の決定木において，学習に使用するデータは元データからランダムに抽出し，決定木ごとに使用するデータが異なるようにする．
      2. <b>ノード分割に使用する特徴量のランダム性</b>
         - ランダムフォレストでは，ノード分割時に全特徴量を使用するのではなく，特徴量の一部をランダム抽出し，抽出した特徴量のみを考慮して最も情報利得の高い分割条件を作成する．
- ランダムフォレストで作成される各決定木は，ノード分割時に一部の特徴量しか考慮しないため，ひとつひとつの決定木のみを考えると識別精度は通常の決定木よりも下がる．
  - しかし，識別精度が下がった複数の決定木を総合的に考えると，最終的な識別精度が上がるのがランダムフォレストの特徴．

### 注意事項
- ランダムフォレストの使用時には注意点が2つある：
   1. 多重共線性を排除しておくこと
      - <b>多重共線性とは特徴量同士で相関が強すぎる状態を意味する（とある変数のペアにおいて相関値0.95以上あたりが目安）．</b>
   2. 過学習について
      - 基本的には深さが深い状態からはじめ，訓練データと検証データの正解率の差がある程度まで小さくなるところまで，深さを浅くしていく（max_depthを徐々に小さくしていく）．
      - この状態からさらにmax_depthを小さくし，深さを浅くすると，正解率の差はあまり変わらないのに，訓練データも検証データもそれぞれ正解率が悪くなっていく．
      - このあたりのバランスを見て，ランダムフォレストで過学習ではない最適なハイパーパラメータを探る．